data_configs:
  lang_pair: "en-de"
  train_data:
    - "/home/user_data55/yuex/Europarl7/docs_data/train.en.doc.bpe.20"
    - "/home/user_data55/yuex/Europarl7/docs_data/train.de.doc.bpe.20"
  valid_data:
    - "/home/user_data55/yuex/Europarl7/docs_data/dev.en.doc.bpe.20"
    - "/home/user_data55/yuex/Europarl7/docs_data/dev.de.doc.bpe.20"
  bleu_valid_reference: "/home/user_data55/yuex/Europarl7/sents_data/dev.de.norm.tok"
  vocabularies:
    - type: "word"
      dict_path: "/home/user_data55/yuex/Europarl7/vocab/vocab.en.bpe.json"
      max_n_words: -1
    - type: "word"
      dict_path: "/home/user_data55/yuex/Europarl7/vocab/vocab.de.bpe.json"
      max_n_words: -1
  max_len:
    - -1
    - -1
  num_refs: 1
  eval_at_char_level: false

model_configs:
  model: D2D
  enable_history_context: true  # decoder: enable transformer-xl mem
  enable_global_encoding: true # encoder: encode entire document or separate sentences
  reset_encoder_position: true # reset position embedding for each sentence
  max_encoder_segment_embedding: 20 # adding segment embedding to each sentence. disable when -1
  encoder_attention_type: normal # [normal, relative]
  global_encoder_attention_type: segment-relative # [none, normal, word-relative, segment-relative]
  global_encoder_gate: true # toggle gating mechanism to fuse local&global output
  global_encoder_cat: false # toggle concatenate mechanism to fuse local&global output
  n_layers: 4
  n_head: 4
  d_word_vec: 256
  d_model: 256
  d_inner_hid: 512
  dropout: 0.1
  proj_share_weight: true
  label_smoothing: 0.1

optimizer_configs:
  optimizer: "adam"
  learning_rate: 0.1
  grad_clip: -1.0
  optimizer_params: ~
  schedule_method: noam
  scheduler_configs:
    d_model: 256
    warmup_steps: 8000

training_configs:
  seed: 1234
  max_epochs: 50
  shuffle: true
  use_bucket: true
  batch_size: 1024
  batching_key: "tokens"
  update_cycle: 2
  disp_freq: 1000
  save_freq: 1000
  num_kept_checkpoints: 1
  loss_valid_freq: 1000
  valid_batch_size: 1
  bleu_valid_freq: 1000
  bleu_valid_batch_size: 1
  bleu_valid_warmup: 12
  bleu_valid_configs:
    max_steps: 150
    beam_size: 5
    alpha: 0.6
    sacrebleu_args: "--tokenize none -lc"
    postprocess: false
  early_stop_patience: 50
